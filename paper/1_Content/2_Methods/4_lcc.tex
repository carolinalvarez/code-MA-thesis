% !TeX program = pdflatex
% !BIB program = biber
\subsection{Local case-control subsampling}
\label{sec:lcc}

Local-case control subsampling (LCC) was proposed by \textcite{hastie2014} as a generalization of the standard case-control subsampling. As opposed to the methods discussed above, LCC allows the acceptance probability function to depend not only on $y$ but also on the covariates $x$. To do so, it uses a pilot estimator of $P(Y=1 \mid X=x)$, which is fitted with an independent dataset from the original sample. Denote the pilot estimate as:

 % \tilde{\theta} = (\tilde{\alpha}, \tilde{\beta})$ of the population parameters $\theta = (\alpha, \beta)$
 
% *In \cite{wang2020rare}, they say that the LCC is based on the assumption that the probability of an event occurring is fixed and does not go to zero. 

\begin{equation}
    \tilde{p}(x_i) = \frac{e^{\tilde{\alpha} + x_i^\prime \tilde{\beta}}}{1 + e^{\tilde{\alpha} + x_i^\prime \tilde{\beta}}}
    \label{eqn:pilot}
\end{equation}

where $(\tilde{\alpha}, \tilde{\beta}) = \tilde{\theta}$. In some cases, there could already be an available pilot fit; for example, when a model is fitted every day with incoming data, then yesterday's fit is a good pilot for today's model (\cite{hastie2014}). However, when no pilot is available, the authors propose a first pass of the WCC algorithm using a fraction of the full sample to estimate $\tilde{p}(x_i)$. \\

Since the WCC estimate has been proved to be $\sqrt{N}$ consistent and asymptotically unbiased, the LCC estimate in the second stage will subsequently be consistent and unbiased, as explained later in this chapter. The authors recommend using a pilot data sample of the same size as the sample used to fit the LCC algorithm. However, it is important to note that this may not always provide a sufficient sample size for the pilot model, especially for small $N$. This will be further discussed in Section \ref{sec:sim_study}, where finite sample guarantees will also be explored.\\

The acceptance probability function $a(x_i, y_i)$ for the LCC is then defined as follows:

\begin{align}
  a(x_i,y_i)=|y-\text{$\tilde{p}(x_i)$}|=
  \begin{cases}
    1-\text{$\tilde{p}(x_i)$}, & \text{if $y=1$}.\\
    \text{$\tilde{p}(x_i)$}, & \text{otherwise}.
  \end{cases}
  \label{eqn:prob-function-lcc}
\end{align}

As before, the adjustment of the estimates can be justified using the derivation of Section \ref{sec:cc}; however, this time, both the intercept and the slope coefficients need to be adjusted. For LCC, the correction factor $b$ is a function of the data, such that $b(x) = \log \left(\frac{a(x_i,1)}{a(x_i,0)} \right)$. Then, from Equation \ref{eqn:coef-adjust} we have:

\begin{align}
    \nonumber \frac{P(y=1 \mid x, d) A_1 B}{1 - P(y=1 \mid x, d) A_1 B} &= \alpha - b(x) + x_i^{\prime} \beta \\
    \nonumber &= \alpha - \log \left(\frac{a(x_i,1)}{a(x_i,0)}\right) + x_i^{\prime} \beta \\ 
    \nonumber &= \alpha - \log \left(\frac{1-\tilde{p}(x_i)}{\tilde{p}(x_i)}\right) + x_i^{\prime} \beta \\
    \nonumber &= \alpha - \log \left(\frac{1}{e^{\tilde{\alpha} + x_i^{\prime} \tilde{\beta}}}\right) + x_i^{\prime} \beta \\
    \nonumber &= \alpha - (-\tilde{\alpha} - x_i^{\prime} \tilde{\beta}) + x_i^{\prime} \beta \\
    &= (\alpha + \tilde{\alpha}) +  x_i^{\prime} (\beta + \tilde{\beta}) = \alpha^* + x_i^{\prime} \beta^*
    \label{eqn:lcc-adjustment}
\end{align}


\begin{algorithm}[ht]
  \caption{LCC subsampling}
  \begin{enumerate}
    \item 
    Generate independent $z_i \sim \operatorname{Bernoulli}\left(a\left(x_i,y_i\right)\right)$, where $z_i$ is generated by:
    \begin{enumerate}
      \item 
      Generate $u_i \sim U(0,1)$, which is independent of the data, the pilot, and each $i$
      \item 
      Create $z_i=\mathbf{1}_{u_i \leq a\left(y_i\right)}$
      \item Generate the subsample $S=\left\{\left(x_i, y_i\right): z_i=1\right\}$ 
    \end{enumerate}
    \item 
    Fit a logistic regression to the subsample $S$ and obtain unadjusted estimates $\hat{\theta}_{S}=(\hat{\alpha}_{S}, \hat{\beta}_{S})$
    \item
    Get adjusted estimates for the population by:
    \begin{enumerate}
        \item $\hat{\alpha} \leftarrow \hat{\alpha}_{S}+ \tilde{\alpha}$
        \item $\hat{\beta} \leftarrow \hat{\beta}_{S} + \tilde{\beta}$
    \end{enumerate}
  \end{enumerate}
  \label{alg:alg_lcc}
\end{algorithm}

The intuition behind LCC is that it measures the degree of ``surprisingness'' of each data point through the difference between the observed $y_i$ and the predicted $\tilde{p}(x_i)$, under the assumption that the pilot estimate accurately approximates $\tilde{p}(x_i)$. The larger the difference, the more likely the data point will be selected for inclusion into $S$. Thus, each subsampled $y_i$ is more informative than a traditional case-control subsampled data point, and valid estimates for the full sample can be obtained by adjusting log-odds using Equation \ref{eqn:lcc-adjustment}.\\

Furthermore, \textcite{hastie2014} argue that one of the main advantages of LCC is that it can fully exploit conditional imbalance, whereas CC and WCC cannot. Thus, we could expect that in scenarios with high levels of conditional imbalance, LCC outperforms the methods in terms of both bias and efficiency. The performance of LCC under marginal imbalance, however, is not discussed by the authors. From the theory, it is still unknown whether LCC will outperform the other methods in cases where there is a high marginal imbalance but a low conditional imbalance. This theoretical gap will later be explored in the numerical exercises of Section \ref{sec:sim_study}.


\subsubsection{Preliminaries for asymptotic results}

For this part of the thesis, I will follow the notation used by \textcite{hastie2014} in the asymptotic section of their paper. I will not present formal proofs but rather the intuition and the underlying assumptions behind the Lemmas, Propositions, and Theorems. For simplicity, let $\lambda$ now denote the pilot estimates $\tilde{\theta}$. Furthermore, the constant term is absorbed to $x$, so that $f_{\lambda} (x_i) = \lambda^{\prime} x_i$. The LCC acceptance probability is now a function of $\lambda$, such that $a_\lambda(x_i, y_i)=\left|y_i-\frac{e^{x_i^{\prime} \lambda}}{1 + e^{x_i^{\prime} \lambda}}\right| \in(0,1)$ and its expected value is $\bar{a}(\lambda) = \mathbb{E} a_{\lambda}(X, Y) \in (0,1)$. The acceptance probability of $x_i$ into the subsample $S$ is:

\begin{equation}
    \hat{a}_{\lambda}(x_i) = \tilde{p}(x_i)(1 - p(x_i)) + (1 - \tilde{p}(x_i))p(x_i) \in (0,1), 
\end{equation}

where $\hat{a}_{\lambda}(x_i)$ is gonna be small if $\tilde{p}(x_i)$ is well approximated but large for data points where the true $p(x_i)$ differs greatly from $\tilde{p}(x_i)$. Additionally, a key assumption throughout this section is the non-separability of classes, as stated in Lemma 1:\\

\textbf{Lemma 1 as stated in \textcite{hastie2014}:}
Assume there is no $v \in \mathbb{R}^p$ for which $P(Y=0, v \prime X>0) = P(Y=1, v \prime X<0) = 0$. Hence, it follows that there is non-separability of classes.\\

This assumption means that there exists some degree of class overlap that does not allow the classes to be perfectly separable by a hyperplane in the feature space. Therefore, some misclassification can always be expected, and the objective is to minimize it. Let then $Q_\lambda(\theta)$ be the population risk function, such that $\hat{\theta}_{LCC} \approx \arg \min_\theta Q_\lambda(\theta)$. Assume further that the model is correctly specified and that the best linear predictor for the full sample is $\theta^* = \bar{\theta}(0)$, which is the large-sample limit of the LCC estimator with pilot fixed at $\lambda = 0$. Then, the authors present the following Proposition: \\

\textbf{Proposition 2 as in \textcite{hastie2014}}: Assume $\mathbb{E}\|X\|<\infty$, the classes are non-separable and that $\theta^* = \bar{\theta}(0)$. Then, $\theta^*=\arg \min _\theta Q_{\theta^*}(\theta)=\bar{\theta}\left(\theta^*\right)$.\\

The main takeaway of Proposition 2 is that it suggests that if the pilot comes near to $\theta^*$, then the LCC estimator will also converge eventually to $\theta^*$. Furthermore, as a WCC estimator, the pilot will be consistent even under model misspecification, and so will the LCC estimate. Note that for this to hold, it must be that both $N \rightarrow \infty$ and the pilot $N_s \rightarrow \infty$, which could be difficult to hold, especially in small samples. 

\subsubsection{Consistency of $\hat{\theta}_{LCC}$}

For the consistency of the LCC estimator, assume that there are infinite data point pairs $(x_i, y_i)$, a sequence of i.i.d uniform variables \{$u_1, u_2, ..., u_n$\} and a sequence of pilot estimates \{$\lambda_1, \lambda_2, ..., \lambda_n$\}. In this section, the assumption of independence of the pilot and the data is not strict, and it actually allows a dependency between the two. However, the sequence of uniform variables for performing the accept-reject decisions are assumed to be independent of the data, the pilot, and themselves. \\

The consistency of $\hat{\theta}_{LCC}$ is built on the asymptotic results of the pilot, specifically on the idea that if the pilot converges in probability to the optimal population parameter, then $\widehat{Q}_{\lambda_n} \approx Q_{\theta^*}$. $\widehat{Q}_{\lambda_n}$ denotes the empirical risk function minimized by LCC, whereas $Q_{\theta^*}$ is the analogous function for the true population, minimized by $\theta^*$. In Proposition 3 of the paper, the authors define and prove pointwise convergence of the sequence of pilot estimates to their asymptotic limit $\widehat{Q}_{\lambda_n}(\theta) \rightarrow Q_{\lambda_{\infty}}(\theta)$, which in turns also implies that uniform convergence on compacts also holds (see Proposition 4 in \textcite{hastie2014}). With these two results, it follows:\\

\textbf{Theorem 5 as in \textcite{hastie2014}
}: Assume $\mathbb{E}\|X\|<\infty$ and that the classes are non-separable. Then, if the pilot estimate is consistent such that $\lambda_n \stackrel{p}{\rightarrow} \theta^*$, then the local case-control estimate will be consistent $\hat{\theta}_n \stackrel{p}{\rightarrow} \theta^*$ as well.

\subsubsection{Asymptotic distribution}
\label{sec:lcc-asymp}

In contrast to the previous section, the authors assume strict independence between the pilot and data to present the results for the theoretical asymptotic distribution of the LCC estimate. To derive the asymptotic variance of the estimate, denote the gradient of $Q_\lambda (\theta)$ as:

\begin{equation}
    G(\theta, \lambda) \triangleq-\bar{a}(\lambda) \nabla_\theta Q_\lambda(\theta),
    \label{eqn:gradient-lcc}
\end{equation}

with variance-covariance matrix:

\begin{equation}
    J(\theta, \lambda) \triangleq \operatorname{Var}_\lambda\left[\left(Y-\frac{e^{X^{\prime}(\theta-\lambda)}}{1+e^{X^{\prime}(\theta-\lambda)}}\right) X\right].
\end{equation}

One can retrieve the Hessian from  Equation \ref{eqn:gradient-lcc} by differentiating again with respect to $\theta$, obtaining:

\begin{equation}
    H(\theta, \lambda) \triangleq-\bar{a}(\lambda) \nabla_\theta^2 Q_\lambda(\theta).
\end{equation}

According to \textcite{hastie2014}, following Maximum Likelihood estimation theory, by fixing the pilot estimates $\lambda$ and approximating the sample size to the limit, $N \rightarrow \infty$, the coefficients of a logistic regression fitted on a sample sized $n\bar{a}(\lambda)$ from the original population would be asymptotically normal, with covariance matrix:

\begin{equation}
    \frac{1}{n\bar{a}(\lambda)} H(\bar{\theta}(\lambda), \lambda)^{-1} J(\bar{\theta}(\lambda), \lambda) H(\bar{\theta}(\lambda), \lambda)^{-1}.
\end{equation}

From these results, it follows:\\

\textbf{Theorem 6 as in \textcite{hastie2014}}: Assume $\mathbb{E}\|X\|^2<\infty$. If the pilot is independent of the data and its estimates are consistent, such that $\lambda_n \stackrel{p}{\rightarrow} \theta^*$, then

\begin{equation}
    \sqrt{n}\left(\hat{\theta}_n-\bar{\theta}\left(\lambda_n\right)\right) \stackrel{d}{\rightarrow} N\left(0, \bar{a}\left(\theta^*\right)^{-1} \Sigma\right)   
\end{equation}

with 

\begin{equation}
    \Sigma=H\left(\theta^*, \theta^*\right)^{-1} J\left(\theta^*, \theta^*\right) H\left(\theta^*, \theta^*\right)^{-1}.
\end{equation}

The proof of Theorem 6 can be found in the Appendix section of \textcite{hastie2014}. Furthermore, from the theorem's derivations, the authors obtain some reassuring facts that emphasize the close asymptotic relation between the pilot and the LCC estimates. These are presented in Corollary 7 of the original paper and briefly summarized below:

\begin{enumerate}
    \item If $\lambda_n$ is $\sqrt{N}$-consistent as a sequence of Horvitz-Thompson estimators, then so is $\hat{\theta}_n$.
    \item If $\lambda_n$ is asymptotically unbiased, then so is $\hat{\theta}_n$.
    \item If $\sqrt{n}\left(\lambda_n-\theta^*\right) \stackrel{d}{\rightarrow} N(0, V)$ then $\sqrt{n}\left(\hat{\theta}_n-\theta^*\right) \stackrel{d}{\rightarrow} N(0, \Sigma)$ with,
    \begin{equation}
        \Sigma=H^{-1}\left(C V C^{\prime}+\bar{a}^{-1} J\right) H^{-1}
        \label{eqn:hastie-57}
    \end{equation}
    where $C$ is a matrix of cross-sectional partial derivatives (for simplicity, the whole expression suppresses the argument of $\theta^*$ as in the original paper). 
\end{enumerate}

Equation \ref{eqn:hastie-57} highlights the fact that the variance of the LCC estimator incorporates the variance of the pilot, denoted as $V$. Thus, we can expect the variance of $\hat{\theta}_{LCC}$ to increase or decrease as the pilot becomes less or more efficient, respectively. Furthermore, under the assumption that the logistic model is correctly specified, the following important theorem holds:\\

\textbf{Theorem 8 as in \textcite{hastie2014}:} Assume that the logistic model is correctly specified and let $\frac{1}{n} \Sigma_{full}$ be the asymptotic variance of the $\theta_{MLE}$ for the full population. If $\mathbb{E}\|X\|^2<\infty$, the pilot is independent of the data, and its estimates are consistent such that $\lambda_n \stackrel{p}{\rightarrow} \theta^*$, then:

\begin{equation}
    \sqrt{n}\left(\hat{\theta}_n-\theta_0\right) \stackrel{\mathcal{D}}{\rightarrow} N\left(0, a\left(\theta_0\right)^{-1} \Sigma\right)=N\left(0,2 \Sigma_{\text {full }}\right)
\end{equation}

Assuming that the model is accurately specified, Theorem 8 offers a robust and important theoretical result for the LCC estimate. The intuition behind it is that even if the LCC takes only $n\bar{a}(\lambda)$ of the full sample data points, its variance is only twice as large as the variance of the logistic regression estimates on the full sample. According to the authors, this means that its variance is the same as the variance of a logistic regression estimate fitted on a random uniform subsample of size $\frac{N}{2}$ from the full population. That is, each point sampled by the LCC method is worth $\frac{1}{2\bar{a}(\lambda)}$ data points taken by uniform sampling. \\

This implies that the smaller the expected value of the true sampling probability $a(\theta_0)$ is, i.e., $\bar{a}\left(\theta_0\right)=\mathbb{E}(|Y-\tilde{p}(X)|)$, the most advantageous the LCC method will be in terms of reducing computational costs. In other words, the more the outcome variable $Y$ is easy to predict throughout the feature space, the smaller the LCC subsample will be, reducing the burden in computational time of estimating $\theta_{MLE}$, yet still only having twice its variance. Again, the assumptions of correct specification of the model and large $N$ must hold.\\

As we will see in the numerical exercises of Section \ref{sec:sim_study}, there are some drawbacks to the LCC method, especially related to its effective subsample size and the consistency of the pilot. In corner cases, even when there is high conditional imbalance, a severe marginal imbalance, i.e., $P(Y=1) > 0.9$, will reduce the effective sample size of LCC drastically. Consequently, the pilot estimate will also suffer from a reduced sample size, failing to reach consistency, and thus, the LCC estimate itself could become extremely inefficient. 
