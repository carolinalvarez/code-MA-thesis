% !TeX program = pdflatex
% !BIB program = biber
\subsection{Weighted case-control subsampling}
\label{sec:wcc}

% Say it was essential to define CC correctly extensively because all the methods are based on that. Here is also important to note that one uses de WCC for the pilot model in the LCC.

An alternative sampling procedure to CC is the weighted case-control subsampling algorithm (WCC), based on the weighted exogenous sampling maximum-likelihood estimator first introduced by \textcite{manski1977estimation}. The authors proved that the WCC estimator is consistent and asymptotically normal if the population probabilities $\tau$ and $(1-\tau)$ are available to the researcher (\cite{maalouf2011logistic}). The main idea is to weight each subsample data point by the inverse of their probability of being sampled, $a(y)^{-1}$ (\cite{hastie2014}). The intuition behind weighting is that if the sampling probability for some event is large, then $a(y)^{-1}$ will be small, and the data points with larger sampling probabilities are given less weight in the parameter's estimation (\cite{maalouf2011logistic}). Therefore, instead of maximizing \ref{eqn:log-likelihood}, the method maximizes:

% \sum_{i=1}^N\left\{\frac{1}{a(1)} y_i \log p\left(x_i\right)+ \frac{1}{a(0)}\left(1-y_i\right) \log \left(1-p\left(x_i \right)\right)\right\} \\ 

\begin{equation}
    \ell(\theta) = \left( \frac{1}{a(1)} \right) \sum_{i=1}^N y_i \log p\left(x_i\right) - \left( \frac{1}{a(0)}\right)\sum_{i=1}^N \left(1-y_i\right) \log \left(1-p\left(x_i \right)\right)
    \label{eqn:w-log-likelihood}.
\end{equation}

\begin{algorithm}[ht]
  \caption{WCC subsampling}
  \begin{enumerate}
    \item 
    Generate independent $z_i \sim \operatorname{Bernoulli}\left(a\left(y_i\right)\right)$, where $z_i$ is generated by:
    \begin{enumerate}
      \item 
      Generate $u_i \sim U(0,1)$, which is independent of the data, the pilot, and each $i$
      \item 
      Create $z_i=\mathbf{1}_{u_i \leq a\left(y_i\right)}$
      \item Generate the subsample $S=\left\{\left(x_i, y_i\right): z_i=1\right\}$ 
    \end{enumerate}
    \item 
    Fit a weighted logistic regression to the subsample $S$ as specified in Equation \ref{eqn:w-log-likelihood} and obtain unadjusted estimates $\hat{\theta}_{S}=(\hat{\alpha}_{S}, \hat{\beta}_{S})$
    \item
    Get unadjusted $\widehat{\theta}_{WCC}$ estimates for the population by:
    \begin{enumerate}
        \item $\hat{\alpha}_{WCC} \leftarrow \hat{\alpha}_{S}$
        \item $\hat{\beta}_{WCC} \leftarrow \hat{\beta}_{S}$
    \end{enumerate}
  \end{enumerate}
  \label{alg:alg_wcc}
\end{algorithm}


Since $a(y_i) > 0$ for every $i \in N$, the WCC estimator is considered a Horvitz-Thompson estimator, and thus, it is an unbiased, $\sqrt{N}$-consistent and asymptotically normal estimator for the population parameter $\theta$ under unequal selection probabilities. (\cite{manski1977estimation}, \cite{scott2002}, \cite{hastie2014}, \cite{overton1995horvitz}). In \textcite{scott2002}, it is mentioned that the appeal of the WCC estimator is its robustness in the case of misspecification of the model. When the linear model in \ref{eqn:w-log-likelihood} does not hold, the WCC estimator can still yield a consistent solution for $\theta$ (\cite{scott2002}). \\

To show this, consider the formulation in \textcite{scott1986} and \textcite{scott2002}. If a random sample of size $N_s$ is taken from the population and we assume the logistic form is valid, then the maximum likelihood estimator $\hat{\theta}_{MLE}$ would satisfy:

\begin{align}
    % \nonumber \sum_{i:y=1} x_i p_0(x_i, \hat{\theta}) - \sum_{i: y=0} x_i p_1(x_i,  \hat{\theta})=0 \\
    \sum_{i:y=1} x_i p_0(x_i,  \hat{\theta}_{MLE}) = \sum_{i: y=0} x_i p_1(x_i,  \hat{\theta}_{MLE})
\end{align}

As $N_s \rightarrow \infty$, then $\hat{\theta}_{MLE}$ converges to $\beta$:

\begin{equation}
    \tau E_1 \{\mathbf{X} p_0(\mathbf{X}, \beta)\} = (1 - \tau) E_0 \{\mathbf{X} p_1(\mathbf{X}, \beta)\}
    \label{eqn:wcc-convergence}
\end{equation}

where $E_1$ and $E_0$ denote the expectation of the conditional distribution of the ($N \times k$) regressors matrix $\mathbf{X}$ given $Y=1$ and $Y=0$, respectively (\cite{scott1986}). The same approach can be written for the weighted estimator using the inverse of the selection probabilities, and thus $\hat{\theta}_{WCC}$ is the solution of:

\begin{equation}
    \sum_{j=1}^{n_1} \frac{x_{1j} p_0(x_{1j},  \hat{\theta}_{WCC})}{a(1)} = \sum_{j=1}^{n_0} \frac{x_{0j} p_1(x_{0j},  \hat{\theta}_{WCC})}{a(0)}.
\end{equation}

where $n_1$ is the number of cases, and $n_0$ is the number of controls in the subsample. According to \textcite{scott1986}, using the weak law of large numbers, one can show that $\hat{\theta}_{WCC}$ also converges to $\theta$ in probability as $n_1, n_0 \rightarrow \infty$. The main message of this result is that even when the linear logistic model is not valid, the $\hat{\theta}_{WCC}$ estimator can still be interpreted as the best-fitting logistic model solution for the full sample. In other words, no matter the form of the true model, $\hat{\theta}_{WCC}$ will converge in probability to $\theta$ (\cite{scott1986}). \\

Despite the method's clear robustness when the model is misspecified, its main drawback is that the variation in the selection probabilities between cases and controls generates a large variance in the parameters' estimators  (\cite{scott2002}, \cite{li2011}). Moreover, the larger the imbalance in the data, the greater the efficiency loss since high disproportions in the data will induce large weights in the sample design (\cite{elliott2000}). Furthermore, the increase in the variance can overpower the potential bias reduction, in general incrementing the Mean Squared Error (MSE) and harming the estimator's prediction performance (\cite{elliott2000}). For a humorous example, one can refer to Basu's elephant tale (\cite{basu1971} pg. 176-177), where a circus statistician faces the dramatic consequences of working with severely noisy sampling probabilities. 





