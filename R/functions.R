#' Author: Carolina Alvarez
#' Code with functions used in the simulation study
#' 
#' 
#' Data Generation Process following Fithian and Hastie. Flexible function for creating an imbalanced data set.
#' 
#' @param N: Integer, size of desired population
#' @param r: Imbalanced ratio, as the rate of class 0, P(Y=0|X=x)
#' @param distribution: Distribution of predictors. Options: Gaussian.
#' @param k : Length of predictor vector.
#' @param mean1 : mean of class 1
#' @param mean0: mean of class 0
#' @param sigma1: variance-covariance of predictors of class1
#' @param sigma0: variance-covariance of predictors of class0
#' 
#' @returns df: imbalanced population

dgp.imbalanced <- function(N
                           , r
                           , distribution
                           , k
                           , mean1
                           , mean0
                           , sigma1
                           , sigma0) {
  
  
  n_class1 <- ceiling(N * (1-r))
  n_class0 <- ceiling(N * r)
  
  y0 <- rep(0, n_class0)
  y1 <- rep(1, n_class1)
  
  if (distribution=="gaussian"){
    X_class1 <- matrix(mvrnorm(n_class1, mu = mean1, Sigma = sigma1, empirical = TRUE), ncol = k)
    X_class1 <- cbind(y1, X_class1)
    
    X_class0 <- matrix(mvrnorm(n_class0, mu = mean0, Sigma = sigma0, empirical = TRUE), ncol = k)
    X_class0 <- cbind(y0, X_class0)
    
    df <- as.data.frame(rbind(X_class1, X_class0))
    
    colnames(df) <- c("y", paste0("X", 1:k))
    
    return(df)
    
  }
  else{
    stop("Distribution not allowed.")
  }
  
}


get.true.intercept <- function(r, x0, betas){
  #' The function allows to recover the true intercept for a given value of r 
  #' the complexity of generating the simulated dataset specifically for fixed 
  #' percentages of r requires b0 values to vary for different r
  #' 
  #' r (numeric, percentage): imbalanced ratio, % of '1'
  #' x0 (numeric or vector): fixed value for X_k
  #' betas (numeric or vector): fixed value for beta_k
  
  
  beta_0 <- log(r) - log(1-r) - t(x0)%*%betas
  return(as.numeric(beta_0))
  
}

get.true.intercept(0.02, c(0.5, 0.5, 0.5), c(1, 1, 1))

#' Case-Control subsampling by Fithian and Hastie 
#'
#' Algorithm for subsampling and fitting a logistic regression in the data
#' hyperparameter a(y): the proportion of 1's we want to subsample. The subsample is generated by first generating 
#' ui ~ U(0,1) that is mutually intedepndent from the pilot (?), the data and each other. Then zi ~ 1 if ui<=a(yi)


cc_algorithm <- function(data, a){
  
  k <- length(data) - 1 # we take "y" out
  
  selection_bias <- log(a/(1-a))
  
  prob_function <- function(data, a){
    
    data$a <- ifelse(data$y == 0, 1 - a, a)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, a)
  
  U <- runif(nrow(data), 0, 1)
  tmp01$U <- U
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  #Subsample
  tmp02 <- tmp01[tmp01$Z==1, ] 
  class_distr_subsample <- table(tmp02$y)
  
  xvars <- paste("X", 1:k, sep="")
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = binomial) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  beta0_adjusted <- coef_unadjusted[1] - selection_bias
  
  coef_adjusted <- c(beta0_adjusted, coef_unadjusted[2:(k+1)])
  
  res <- list("subsample_cc" = tmp02
              , "coef_unadjusted" = coef_unadjusted
              , "coef_adjusted" = coef_adjusted
              )
  
  return(res)
  
}

cc_algorithm_Ns <- function(data, a){
  
  k <- length(data) - 1 # we take "y" out
  
  selection_bias <- log(a/(1-a))
  
  xvars <- paste("X", 1:k, sep="")
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= data
                         , family = binomial) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  beta0_adjusted <- coef_unadjusted[1] - selection_bias
  
  coef_adjusted <- c(beta0_adjusted, coef_unadjusted[2:(k+1)])
  
  res <- list("subsample_S" = data
              , "coef_unadjusted" = coef_unadjusted
              , "coef_adjusted" = coef_adjusted
  )
  
  return(res)
  
}


#' Prediction function
#' A flexible function of predicting odds logistic regression framework
#' 
logit_predict <- function(data, names.use, betas){
  # data (dataframe): test set
  # names.use (vector): vector containing strings with names of features (columns of 'data')
  # betas (vector): vector containing beta coefficients, including intercept (IMP for Fithian and Hastie (2014))
  # For cc, betas is the vector of adjusted coef
  
  X <- as.matrix(cbind(rep(1), data[, names.use]))
  colnames(X) <- c("intercept", names.use)
  n <- nrow(X)
  p <- ncol(X)
  p1 <- seq_len(p) 
  beta <- betas 
  predictor_vector <- 1/(1+exp((-1)*drop(X[, p1, drop = FALSE] %*% beta[p1])))
  return(predictor_vector)
  
}




#' Stratified hold-out method / Generation of train and test sets
#' Function assures that class distribution p(x) of 'y' remains

strat_sampling <- function(data, split_criteria){
  #' data (dataframe): data to split into train and test set
  #' split_criteria (numeric): percentage of observations assigned to the train set
  
  train_idx <- c()
  test_idx <- c()
  
  for (i in c('0', '1')) {
    
    idx <- which(data$y == i)
    n_train <- round(length(idx) * split_criteria) 
    
    train_idx_i <- sample(idx, n_train)
    test_idx_i <- setdiff(idx, train_idx_i)
    train_idx <- c(train_idx, train_idx_i)
    test_idx <- c(test_idx, test_idx_i)
  
    }
  
  # Create train and test sets
  df_train <- data[train_idx, ]
  df_test <- data[test_idx, ]
  
  res <- list("df_train" = df_train
              , "df_test" = df_test)
  
  return(res)
  
}

# Proving I get the same prob and ratios as equations for CC

a_bar <- function(a, r){
  a_bar <- a*(1-r) + (1-a) * r
  return(a_bar)
}

prop_Ps <-function(a, r){
  a1 <- a
  a0 <- 1-a
  
  prop_Ps_1 <- (a1 * (1-r))/ (a1*(1-r) + a0*r)
  prop_Ps_0 <- (a0 * r)/ (a1*(1-r) + a0*r)
  
  c <- c(prop_Ps_0, prop_Ps_1)
  
  return(c)
}


# Function for taking N_s


gdp.imbalanced.Ns <- function(a
                              , r
                              , distribution
                              , Ns_size
                              , k
                              , mean1
                              , mean0
                              , sigma1
                              , sigma0){

  prop_Ps <- prop_Ps(a, r)
  
  prop_Ps_1 <- as.numeric(prop_Ps[2])
  prop_Ps_1 <- round(prop_Ps_1, 1)
  
  prop_Ps_0 <- as.numeric(prop_Ps[1])
  prop_Ps_0 <- round(prop_Ps_0, 1)
  
  N <- Ns_size
  
  # now the usual dgp
  n_class1 <- ceiling(N * prop_Ps_1)
  n_class0 <- ceiling(N * prop_Ps_0)
  
  y0 <- rep(0, n_class0)
  y1 <- rep(1, n_class1)
  
  if (distribution=="gaussian"){
    
    X_class1 <- matrix(mvrnorm(n_class1, mu = mean1, Sigma = sigma1, empirical = TRUE), ncol = k)
    X_class1 <- cbind(y1, X_class1)
    
    X_class0 <- matrix(mvrnorm(n_class0, mu = mean0, Sigma = sigma0, empirical = TRUE), ncol = k)
    X_class0 <- cbind(y0, X_class0)
    
    df <- as.data.frame(rbind(X_class1, X_class0))
    
    colnames(df) <- c("y", paste0("X", 1:k))
    
    return(df)
    
  }
  else{
    stop("Distribution not allowed.")
  }
  
  
}


# Function for WCC sampling 
# 1st. Trial: Not adjusting the coefficient of the intercept (it does not seem like they do...)


wcc_algorithm <- function(data, a){
  
  k <- length(data) - 1 # we take "y" out
  
  selection_bias <- log(a/(1-a))
  
  prob_function <- function(data, a){
    
    data$a <- ifelse(data$y == 0, 1 - a, a)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, a)
  
  U <- runif(nrow(data), 0, 1)
  tmp01$U <- U
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  #Subsample
  tmp02 <- tmp01[tmp01$Z==1, ] 
  class_distr_subsample <- table(tmp02$y)
  
  #weights vector
  tmp02$w <- 1/tmp02$a
  
  
  xvars <- paste("X", 1:k, sep="")
  
  # https://github.com/alan-turing-institute/PosteriorBootstrap/issues/16 on why to use quasibinomial instead of binomial
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = quasibinomial()
                         , weights = w) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  res <- list("subsample_wcc" = tmp02
              , "coef_unadjusted" = coef_unadjusted
  )
  
  return(res)
  
}
  


wcc_algorithm_Ns <- function(data, a){
  
  k <- length(data) - 1 # we take "y" out
  
  selection_bias <- log(a/(1-a))
  
  prob_function <- function(data, a){
    
    data$a <- ifelse(data$y == 0, 1 - a, a)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, a)
  
  #weights vector
  tmp01$w <- 1/tmp01$a
  
  
  xvars <- paste("X", 1:k, sep="")
  
  # https://github.com/alan-turing-institute/PosteriorBootstrap/issues/16 on why to use quasibinomial instead of binomial
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp01
                         , family = quasibinomial()
                         , weights = w) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  res <- list("subsample_S" = tmp01
              , "coef_unadjusted" = coef_unadjusted
  )
  
  return(res)
  
}


lcc_algorithm <- function(data_train, a_wcc){
  
  k <- length(data_train) - 1 # we take "y" out
  
  #here, despite using the strat_sampling function that is built to separate 
  #into train and test, we use this split as two training sets for each algorithm.
  # The only thing i want to make sure is that the split is stratified.
  data_split <- strat_sampling(data_train, 0.5)
  data_wcc <- data_split$df_train
  data_lcc <- data_split$df_test
  
  #run the pilot
  wcc_output <- wcc_algorithm(data_wcc, a_wcc)
  subsample_pilot <- wcc_output$subsample_wcc
  coef_unadjusted_wcc <- wcc_output$coef_unadjusted
  
  #predict on LCC data
  y_hat <- logit_predict(data_lcc, c(paste0("X", 1:k)), coef_unadjusted_wcc)
  
  prob_function <- function(data, y_hat){
  
    data$a <- ifelse(data$y == 0, y_hat, 1-y_hat)
    
    return(data)
  }
  
  tmp01 <- prob_function(data_lcc, y_hat)
  
  U <- runif(nrow(tmp01), 0, 1) # TO DO: in CC instead of tmp01 I wrote data... same?
  
  tmp01$U <- U
  
  a_bar_lcc <- tmp01$a
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  #Subsample
  tmp02 <- tmp01[tmp01$Z==1, ] 
  
  xvars <- paste("X", 1:k, sep="")
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = binomial) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted_lcc <- as.vector(model_subsample$coefficients)
  
  # Now, all coeficients get adjusted, not just the intercept
  coef_adjusted_lcc <- coef_unadjusted_lcc + coef_unadjusted_wcc
  
  
  res <- list("subsample_lcc" = tmp02
              , "subsample_pilot" = subsample_pilot
              , "coef_unadjusted" = coef_unadjusted_lcc
              , "coef_adjusted" = coef_adjusted_lcc
              , "a_bar_lcc" = a_bar_lcc
  )

  return(res)

}


lcc_algorithm_v2 <- function(data = NULL, a_wcc = NULL){
  #' the difference here with the lcc_algorithm() function is that here I do not
  #' further split the data into data for the pilot and for the lcc algorithm.
  #' I think this version of the function is actually what the authors did in 
  #' the paper.
  
  k <- length(data) - 1 # we take "y" out
  #here I do not split the data. I do a wcc pass on the whole data for the pilot
  
  #run the pilot
  wcc_output <- wcc_algorithm(data, a_wcc) #it is gonna be another subsample 
  subsample_pilot <- wcc_output$subsample_wcc
  coef_unadjusted_wcc <- wcc_output$coef_unadjusted
  
  #predict on LCC data
  y_hat <- logit_predict(data, c(paste0("X", 1:k)), coef_unadjusted_wcc)
  
  prob_function <- function(data, y_hat){
    
    data$a <- ifelse(data$y == 0, y_hat, 1-y_hat)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, y_hat)
  
  U <- runif(nrow(tmp01), 0, 1) # TO DO: in CC instead of tmp01 I wrote data... same?
  
  tmp01$U <- U
  
  a_bar_lcc <- tmp01$a
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  #Subsample
  tmp02 <- tmp01[tmp01$Z==1, ] 
  
  xvars <- paste("X", 1:k, sep="")
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = binomial) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted_lcc <- as.vector(model_subsample$coefficients)
  
  # Now, all coeficients get adjusted, not just the intercept
  coef_adjusted_lcc <- coef_unadjusted_lcc + coef_unadjusted_wcc
  
  
  res <- list("subsample_lcc" = tmp02
              , "subsample_pilot" = subsample_pilot
              , "coef_unadjusted" = coef_unadjusted_lcc
              , "coef_adjusted" = coef_adjusted_lcc
              , "a_bar_lcc" = a_bar_lcc
  )
  
  return(res)
  
}



############## Functions with fixed subsample size

cc_algorithm_fixed <- function(data, r, a, ns_fixed){
  
  k <- length(data) - 1 # we take "y" out
  
  selection_bias <- log(a/(1-a))
  # Directly sample from data to generate tmp02_fixed
  n <- nrow(data)
  a1 <- a
  a0 <- 1-a
  prop_1_fixed <- (a1 * (1-r))/ (a1*(1-r) + a0*r)
  
  n_1 <- round(prop_1_fixed * ns_fixed)
  n_0 <- ns_fixed - n_1
  
  idx_1 <- sample(which(data$y == 1), n_1, replace = FALSE)
  idx_0 <- sample(which(data$y == 0), n_0, replace = FALSE)
  
  tmp02_fixed <- rbind(data[idx_1, ],
                       data[idx_0, ])
  
  xvars <- paste("X", 1:k, sep="")
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02_fixed
                         , family = binomial) 
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  beta0_adjusted <- coef_unadjusted[1] - selection_bias
  
  coef_adjusted <- c(beta0_adjusted, coef_unadjusted[2:(k+1)])
  
  res <- list("subsample_cc" = tmp02_fixed
              , "coef_unadjusted" = coef_unadjusted
              , "coef_adjusted" = coef_adjusted
  )
  
  return(res)
  
}


wcc_algorithm_fixed <- function(data, r, a, ns_fixed){
  
  k <- length(data) - 1 # we take "y" out
  
  selection_bias <- log(a/(1-a))
  
  prob_function <- function(data, a){
    
    data$a <- ifelse(data$y == 0, 1 - a, a)
    
    return(data)
  }
  
  data <- prob_function(data, a)

  #weights vector
  data$w <- 1/data$a
  
  #subsample
  n <- nrow(data)
  a1 <- a
  a0 <- 1-a
  prop_1_fixed <- (a1 * (1-r))/ (a1*(1-r) + a0*r)
  
  n_1 <- round(prop_1_fixed * ns_fixed)
  n_0 <- ns_fixed - n_1
  
  idx_1 <- sample(which(data$y == 1), n_1, replace = FALSE)
  idx_0 <- sample(which(data$y == 0), n_0, replace = FALSE)
  
  tmp02_fixed <- rbind(data[idx_1, ],
                       data[idx_0, ])
  
  xvars <- paste("X", 1:k, sep="")
  
  # https://github.com/alan-turing-institute/PosteriorBootstrap/issues/16 on why to use quasibinomial instead of binomial
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02_fixed
                         , family = quasibinomial()
                         , weights = w) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  res <- list("subsample_wcc" = tmp02_fixed
              , "coef_unadjusted" = coef_unadjusted
  )
  
  return(res)
  
}


lcc_algorithm_fixed <- function(data, r, a_wcc, ns_fixed){
  
  k <- length(data) - 1 # we take "y" out
  
  #run the pilot
  wcc_output <- wcc_algorithm_fixed(data, r, a_wcc, ns_fixed)
  subsample_pilot <- wcc_output$subsample_wcc
  coef_unadjusted_wcc <- wcc_output$coef_unadjusted
  
  #predict on LCC data
  y_hat <- logit_predict(data, c(paste0("X", 1:k)), coef_unadjusted_wcc) # TODO: exportar esto tb
  
  prob_function <- function(data, y_hat){
    
    data$a <- ifelse(data$y == 0, y_hat, 1-y_hat)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, y_hat)
  
  U <- runif(nrow(tmp01), 0, 1)
  
  tmp01$U <- U
  
  a_bar_lcc <- tmp01$a
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  tmp02 <- tmp01[tmp01$Z==1, ] 
  
  #Subsample, making sure that the same proportions of tmp02 are in tmp02_fixed
  
  if (sum(tmp01$Z == 1) <= ns_fixed) {
    tmp02_fixed <- tmp01[tmp01$Z == 1, ]
  } else {
    # randomly sample ns_fixed data points from tmp02
    tmp02_fixed <- tmp02[sample(nrow(tmp02), ns_fixed, replace = FALSE,
                                prob = ifelse(tmp02$y == 1, sum(tmp02$y == 1), 
                                              sum(tmp02$y == 0))/nrow(tmp02)), ]
  }
  
  xvars <- paste("X", 1:k, sep="")
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02_fixed
                         , family = binomial) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted_lcc <- as.vector(model_subsample$coefficients)
  
  # Now, all coeficients get adjusted, not just the intercept
  coef_adjusted_lcc <- coef_unadjusted_lcc + coef_unadjusted_wcc
  
  
  res <- list("subsample_lcc" = tmp02
              , "subsample_lcc_fixed" = tmp02_fixed
              , "subsample_pilot" = subsample_pilot
              , "coef_unadjusted" = coef_unadjusted_lcc
              , "coef_adjusted" = coef_adjusted_lcc
              , "a_bar_lcc" = a_bar_lcc
  )
  
  return(res)
}


# function for getting average subsample size given a and r for LCC

average_subsample_size <- function(N, k, a, r, mean1, mean0, sigma1, sigma0, sim){
  res <- NA
  
  for (i in 1:sim) {
    
    df_test <- dgp.imbalanced(N=N, r=r, distribution="gaussian", 
                              k=k, mean1 = mean1, mean0 = mean0, sigma1 = cov_mat, sigma0 = cov_mat)
    
    output_test <- lcc_algorithm_v2(data=df_test, a_wcc = a) # lcc_algorithm_v2!
    lcc_size <- nrow(output_test$subsample_lcc)
    res[i] <- lcc_size
    
  }
  
  return(summary(res))
  
  
}

###############################################################################
###################### Functions for specific sim studies #####################
###############################################################################




monte_carlo_runnings_sim_3_4 <- function(sim = NULL, k = NULL, N = NULL, r = NULL, a = NULL
                                         , ns_fixed1 = NULL, ns_fixed2 = NULL, path_output = NULL
                                         , name_res = NULL){
  
  mean1 <- c(rep(1, k/2), rep(0, k/2))
  mean0 <- c(rep(0, k))
  cov_mat <- diag(k)
  
  beta_names_cc <- paste0("β_hat_cc_", 0:k)
  beta_names_wcc <- paste0("β_hat_wcc_", 0:k)
  beta_names_lcc <- paste0("β_hat_lcc_", 0:k)
  beta_names_logit <- paste0("β_hat_logit_", 0:k)
  
  output <- c(beta_names_cc, beta_names_wcc, beta_names_lcc, beta_names_logit)
  
  res <- data.frame(matrix(ncol = length(output), nrow = 0))
  colnames(res) <- output
  
  for (i in 1:sim) {
    
    df <- dgp.imbalanced(N = N, r = r, distribution= "gaussian", k = k, mean1 = mean1
                         , mean0 = mean0, sigma1 = cov_mat, sigma0 = cov_mat)
    
    
    cc_output <- cc_algorithm_fixed(data=df, r=r, a=a, ns_fixed=ns_fixed1)
    coef_adjusted_cc <- cc_output$coef_adjusted
    
    wcc_output <- wcc_algorithm_fixed(data=df, r=r, a=a, ns_fixed = ns_fixed1)
    coef_unadjusted_wcc <- wcc_output$coef_unadjusted
    
    lcc_output <- lcc_algorithm_fixed(data=df, r=r, a_wcc=a, ns_fixed = ns_fixed2)
    coef_adjusted_lcc <- lcc_output$coef_adjusted
    
    logit_output <- glm(y~., data = df, family = binomial)
    coef_logit <- logit_output$coefficients
    
    a_bar_lcc <- mean(lcc_output$a_bar_lcc)
    
    
    # Coefficients
    res_cc <- data.frame(t(coef_adjusted_cc))
    colnames(res_cc) <- beta_names_cc
    
    res_wcc <- data.frame(t(coef_unadjusted_wcc))
    colnames(res_wcc) <- beta_names_wcc
    
    res_lcc <- data.frame(t(coef_adjusted_lcc))
    colnames(res_lcc) <- beta_names_lcc
    
    res_logit <- data.frame(t(coef_logit))
    colnames(res_logit) <- beta_names_logit
    
    lcc_subsample <- lcc_output$subsample_lcc
    count_controls <- as.numeric(table(lcc_subsample$y)[1])
    count_cases <- as.numeric(table(lcc_subsample$y)[2])
    
    res <- rbind(res
                 , cbind(res_cc, res_wcc, res_lcc, res_logit, 
                         a_bar_lcc, count_controls , count_cases))
    
  }
  
  write.csv(res, file = paste0(path_output, name_res), row.names = TRUE)
  
  res <- res
  
  return(res)
  
}


res_analysis_sim3 <- function(res=NULL, k = NULL, a = NULL){
  
  means <- data.frame(t(colMeans(res)))
  colnames(means) <- gsub("β_hat_", "", colnames(means))
  
  
  # True coefficient values
  beta_true <- c(get.true.intercept(1-r, rep(0.5, k), c(rep(1,k/2), rep(0, k/2))), rep(1, k/2)
                 , rep(0, k/2))
  
  beta_true <- rep(beta_true, 4)
  
  # Calculate squared bias
  squared_bias <- (means - beta_true)^2
  
  # Add column names to squared_bias
  colnames(squared_bias) <- colnames(means)
  
  # Display squared_bias
  squared_bias_cc <- sum(squared_bias[1:as.numeric(k+1)])
  squared_bias_wcc <- sum(squared_bias[as.numeric(k+2):as.numeric(k+k+2)])
  squared_bias_lcc <- sum(squared_bias[as.numeric(k+k+3):as.numeric(k+k+k+3)])
  squared_bias_logit <- sum(squared_bias[as.numeric(k+k+k+5):length(squared_bias)-1])
  
  mean_a_bar <- mean(res$a_bar_lcc)
  mean_a_bar
  
  # Take the variance of the realizations
  variances <- apply(res, 2, var)
  
  var_cc <- sum(variances[1:as.numeric(k+1)])
  var_wcc <- sum(variances[as.numeric(k+2):as.numeric(k+k+2)])
  var_lcc <- sum(variances[as.numeric(k+k+3):as.numeric(k+k+k+3)])
  var_logit <- sum(variances[as.numeric(k+k+k+5):length(squared_bias)-1])
  
  res <- list("beta_true" = beta_true,
              "squared_bias_cc" = squared_bias_cc,
              "squared_bias_wcc" = squared_bias_wcc,
              "squared_bias_lcc" = squared_bias_lcc,
              "squared_bias_logit" = squared_bias_logit,
              "mean_a_bar" = mean_a_bar,#
              "var_cc" = var_cc,
              "var_wcc" <- var_wcc,
              "var_lcc" <- var_lcc, 
              "var_logit" <- var_logit)
  
}



######################### DATA APPLICATION VERSIONS ###########################

cc_algorithm_data <- function(data=NULL
                         , a = NULL
                         , xvars = NULL){
  
  k <- length(data) - 1 
  
  selection_bias <- log(a/(1-a))
  
  prob_function <- function(data, a){
    
    data$a <- ifelse(data$y == 0, 1 - a, a)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, a)
  
  U <- runif(nrow(data), 0, 1)
  tmp01$U <- U
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  #Subsample
  tmp02 <- tmp01[tmp01$Z==1, ] 
  class_distr_subsample <- table(tmp02$y)
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = binomial) 
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  beta0_adjusted <- coef_unadjusted[1] - selection_bias
  
  coef_adjusted <- c(beta0_adjusted, coef_unadjusted[2:(k+1)])
  
  res <- list("subsample_cc" = tmp02
              , "coef_unadjusted" = coef_unadjusted
              , "coef_adjusted" = coef_adjusted
  )
  
  return(res)
  
}


wcc_algorithm_data <- function(data=NULL
                               , a=NULL
                               , xvars = NULL){
  
  k <- length(data) - 1 
  
  selection_bias <- log(a/(1-a))
  
  prob_function <- function(data, a){
    
    data$a <- ifelse(data$y == 0, 1 - a, a)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, a)
  
  U <- runif(nrow(data), 0, 1)
  tmp01$U <- U
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  #Subsample
  tmp02 <- tmp01[tmp01$Z==1, ] 
  class_distr_subsample <- table(tmp02$y)
  
  #weights vector
  tmp02$w <- 1/tmp02$a
  
  # https://github.com/alan-turing-institute/PosteriorBootstrap/issues/16 on why to use quasibinomial instead of binomial
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = quasibinomial()
                         , weights = w) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  res <- list("subsample_wcc" = tmp02
              , "coef_unadjusted" = coef_unadjusted
  )
  
  return(res)
  
}


lcc_algorithm_V2_data<- function(data = NULL
                                       , a_wcc = NULL
                                       , xvars = NULL){
  #' From lcc_algorithm_v2
  
  k <- length(data) - 1 
  
  #run the pilot
  wcc_output <- wcc_algorithm_data(data=data, a=a_wcc, xvars=xvars)  
  subsample_pilot <- wcc_output$subsample_wcc
  coef_unadjusted_wcc <- wcc_output$coef_unadjusted
  
  #predict on LCC data
  y_hat <- logit_predict(data, xvars, coef_unadjusted_wcc)
  
  prob_function <- function(data, y_hat){
    
    data$a <- ifelse(data$y == 0, y_hat, 1-y_hat)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, y_hat)
  
  U <- runif(nrow(tmp01), 0, 1) 
  
  tmp01$U <- U
  
  a_bar_lcc <- tmp01$a
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  #Subsample
  tmp02 <- tmp01[tmp01$Z==1, ] 
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = binomial) 
  
  coef_unadjusted_lcc <- as.vector(model_subsample$coefficients)

  coef_adjusted_lcc <- coef_unadjusted_lcc + coef_unadjusted_wcc
  
  
  res <- list("subsample_lcc" = tmp02
              , "subsample_pilot" = subsample_pilot
              , "coef_unadjusted" = coef_unadjusted_lcc
              , "coef_adjusted" = coef_adjusted_lcc
              , "a_bar_lcc" = a_bar_lcc
  )
  
  return(res)
  
}


cc_algorithm_fixed_data <- function(data=NULL
                               , a = NULL
                               , r = NULL
                               , xvars=NULL
                               , ns_fixed=NULL){
  
  k <- length(data) - 1 
  
  selection_bias <- log(a/(1-a))
  n <- nrow(data)
  a1 <- a
  a0 <- 1-a
  prop_1_fixed <- (a1 * (1-r))/ (a1*(1-r) + a0*r)
  
  n_1 <- round(prop_1_fixed * ns_fixed)
  n_0 <- ns_fixed - n_1
  
  idx_1 <- sample(which(data$y == 1), n_1, replace = FALSE)
  idx_0 <- sample(which(data$y == 0), n_0, replace = FALSE)
  
  tmp02_fixed <- rbind(data[idx_1, ],
                       data[idx_0, ])
  
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02_fixed
                         , family = binomial) 
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  beta0_adjusted <- coef_unadjusted[1] - selection_bias
  
  coef_adjusted <- c(beta0_adjusted, coef_unadjusted[2:(k+1)])
  
  res <- list("subsample_cc" = tmp02_fixed
              , "coef_unadjusted" = coef_unadjusted
              , "coef_adjusted" = coef_adjusted
  )
  
  return(res)
  
}


wcc_algorithm_fixed_data <- function(data=NULL
                                , a = NULL
                                , r = NULL
                                , xvars=NULL
                                , ns_fixed=NULL){
  
  k <- length(data) - 1 
  
  selection_bias <- log(a/(1-a))
  
  prob_function <- function(data, a){
    
    data$a <- ifelse(data$y == 0, 1 - a, a)
    
    return(data)
  }
  
  data <- prob_function(data, a)
  
  #weights vector
  data$w <- 1/data$a
  
  #subsample
  n <- nrow(data)
  a1 <- a
  a0 <- 1-a
  prop_1_fixed <- (a1 * (1-r))/ (a1*(1-r) + a0*r)
  
  n_1 <- round(prop_1_fixed * ns_fixed)
  n_0 <- ns_fixed - n_1
  
  idx_1 <- sample(which(data$y == 1), n_1, replace = FALSE)
  idx_0 <- sample(which(data$y == 0), n_0, replace = FALSE)
  
  tmp02_fixed <- rbind(data[idx_1, ],
                       data[idx_0, ])
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02_fixed
                         , family = quasibinomial()
                         , weights = w) 
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  res <- list("subsample_wcc" = tmp02_fixed
              , "coef_unadjusted" = coef_unadjusted
  )
  
  return(res)
  
}


lcc_algorithm_fixed_data <- function(data = NULL
                                , r = NULL
                                , a_wcc = NULL
                                , xvars=NULL
                                , ns_fixed = NULL){
  
  k <- length(data) - 1 
  
  #run the pilot
  wcc_output <- wcc_algorithm_fixed_data(data = data, a = a_wcc, r=r, xvars = xvars
                                         , ns_fixed = ns_fixed)
  subsample_pilot <- wcc_output$subsample_wcc
  coef_unadjusted_wcc <- wcc_output$coef_unadjusted
  
  #predict on LCC data
  y_hat <- logit_predict(data, xvars, coef_unadjusted_wcc) # TODO: exportar esto tb
  
  prob_function <- function(data, y_hat){
    
    data$a <- ifelse(data$y == 0, y_hat, 1-y_hat)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, y_hat)
  
  U <- runif(nrow(tmp01), 0, 1)
  
  tmp01$U <- U
  
  a_bar_lcc <- tmp01$a
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  tmp02 <- tmp01[tmp01$Z==1, ] 
  
  #Subsample, making sure that the same proportions of tmp02 are in tmp02_fixed
  
  if (sum(tmp01$Z == 1) <= ns_fixed) {
    tmp02_fixed <- tmp01[tmp01$Z == 1, ]
  } else {
    # randomly sample ns_fixed data points from tmp02
    tmp02_fixed <- tmp02[sample(nrow(tmp02), ns_fixed, replace = FALSE,
                                prob = ifelse(tmp02$y == 1, sum(tmp02$y == 1), 
                                              sum(tmp02$y == 0))/nrow(tmp02)), ]
  }
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02_fixed
                         , family = binomial) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted_lcc <- as.vector(model_subsample$coefficients)
  
  # Now, all coeficients get adjusted, not just the intercept
  coef_adjusted_lcc <- coef_unadjusted_lcc + coef_unadjusted_wcc
  
  
  res <- list("subsample_lcc" = tmp02
              , "subsample_lcc_fixed" = tmp02_fixed
              , "subsample_pilot" = subsample_pilot
              , "coef_unadjusted" = coef_unadjusted_lcc
              , "coef_adjusted" = coef_adjusted_lcc
              , "a_bar_lcc" = a_bar_lcc
  )
  
  return(res)
}

## Adapting the functions so that a(y) > 1. This only works if we want to do a 50-50
# split of the data

cc_algorithm_data_2 <- function(data=NULL
                              , a1 = NULL
                              , r = NULL
                              #, a0 = NULL
                              , xvars = NULL){
  
  k <- length(data) - 1 
  
  a1 <- a1
  
  a0 <- ((1-r)*a1)/r
  
  selection_bias <- log(a1/a0)
  
  prob_function <- function(data, a0, a1){
    
    data$a <- ifelse(data$y == 0, a0, a1)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, a0, a1)
  
  U <- runif(nrow(data), 0, 1)
  tmp01$U <- U
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  #Subsample
  tmp02 <- tmp01[tmp01$Z==1, ] 
  class_distr_subsample <- table(tmp02$y)
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = binomial) 
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  beta0_adjusted <- coef_unadjusted[1] - selection_bias
  
  coef_adjusted <- c(beta0_adjusted, coef_unadjusted[2:(k+1)])
  
  res <- list("subsample_cc" = tmp02
              , "coef_unadjusted" = coef_unadjusted
              , "coef_adjusted" = coef_adjusted
  )
  
  return(res)
  
}


wcc_algorithm_data_2 <- function(data=NULL
                               , a1=NULL
                               #, a0=NULL
                               , r = NULL
                               , xvars = NULL){
  
  k <- length(data) - 1 
  
  a1 <- a1
  
  a0 <- ((1-r)*a1)/r
  
  selection_bias <- log(a1/a0)
  
  prob_function <- function(data, a0, a1){
    
    data$a <- ifelse(data$y == 0, a0, a1)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, a0, a1)
  
  U <- runif(nrow(data), 0, 1)
  tmp01$U <- U
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  #Subsample
  tmp02 <- tmp01[tmp01$Z==1, ] 
  class_distr_subsample <- table(tmp02$y)
  
  #weights vector
  tmp02$w <- 1/tmp02$a
  
  # https://github.com/alan-turing-institute/PosteriorBootstrap/issues/16 on why to use quasibinomial instead of binomial
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = quasibinomial()
                         , weights = w) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  res <- list("subsample_wcc" = tmp02
              , "coef_unadjusted" = coef_unadjusted
  )
  
  return(res)
  
}



cc_algorithm_fixed_data_2 <- function(data=NULL
                                     , a1 = NULL
                                     #, a0 = NULL
                                     , r = NULL
                                     , xvars=NULL
                                     , ratio_to = NULL){
  
  k <- length(data) - 1 
  
  n <- nrow(data)
  
  a1 <- a1
  a0 <- ((1-r)*a1)/r
  
  selection_bias <- log(a1/a0)
  
  ns_fixed = a1*(1-r)*nrow(data)*(1/ratio_to)
  n_1 <- round(a1*(1-r)*n)
  n_0 <- ns_fixed - n_1 + 1
  
  idx_1 <- sample(which(data$y == 1), n_1, replace = FALSE)
  idx_0 <- sample(which(data$y == 0), n_0, replace = FALSE)
  
  tmp02_fixed <- rbind(data[idx_1, ],
                       data[idx_0, ])
  
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02_fixed
                         , family = binomial) 
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  beta0_adjusted <- coef_unadjusted[1] - selection_bias
  
  coef_adjusted <- c(beta0_adjusted, coef_unadjusted[2:(k+1)])
  
  res <- list("subsample_cc" = tmp02_fixed
              , "coef_unadjusted" = coef_unadjusted
              , "coef_adjusted" = coef_adjusted
  )
}



wcc_algorithm_fixed_data_2 <- function(data=NULL
                                      , a1 = NULL
                                      #, a0 = NULL
                                      , r = NULL
                                      , xvars=NULL
                                      , ratio_to = NULL){
  
  k <- length(data) - 1 
  
  n <- nrow(data)
  
  a1 <- a1
  a0 <- ((1-r)*a1)/r
  
  prob_function <- function(data, a0, a1){
    
    data$a <- ifelse(data$y == 0, a0, a1)
    
    return(data)
  }
  
  data <- prob_function(data, a0, a1)
  
  #weights vector
  data$w <- 1/data$a
  
  selection_bias <- log(a1/a0)
  
  ns_fixed = a1*(1-r)*nrow(data)*(1/ratio_to)
  n_1 <- a1*(1-r)*n
  n_0 <- ns_fixed - n_1 + 1
  
  idx_1 <- sample(which(data$y == 1), n_1, replace = FALSE)
  idx_0 <- sample(which(data$y == 0), n_0, replace = FALSE)
  
  tmp02_fixed <- rbind(data[idx_1, ],
                       data[idx_0, ])
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02_fixed
                         , family = quasibinomial()
                         , weights = w) 
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  res <- list("subsample_wcc" = tmp02_fixed
              , "coef_unadjusted" = coef_unadjusted
  )
  
  return(res)
  
}



average_subsample_size_data <- function(data=NULL
                                        , a=NULL
                                        , r = NULL
                                        , xvars = NULL
                                        , rep = NULL
                                        , algorithm = NULL
                                        , type = c("a-fixed", "a-flexible")
                                        , a1 = NULL){
  
  if(algorithm == "cc" && type == "a-fixed" && missing(a1) && missing(r)){
    res <- NA
    
    for (i in 1:rep) {
      
      output_test <- cc_algorithm_data(data=data, a = a, xvars=xvars) 
      cc_size <- nrow(output_test$subsample_cc)
      res[i] <- cc_size
      
    }
    
    return(summary(res))
  } else if(algorithm == "cc" && type == "a-flexible" && missing(a)){
    
    res <- NA
    
    for (i in 1:rep) {
      
      output_test <- cc_algorithm_data_2(data=data, a1 = a1, r = r, xvars=xvars) 
      cc_size <- nrow(output_test$subsample_cc)
      res[i] <- cc_size
      
    }
    
    return(summary(res))
  } else if(algorithm == "wcc" && type == "a-fixed" && missing(a1) && missing(r)){
    
    res <- NA
    
    for (i in 1:rep) {
      
      output_test <- wcc_algorithm_data(data=data, a = a, xvars=xvars) 
      wcc_size <- nrow(output_test$subsample_wcc)
      res[i] <- wcc_size
      
    }
    
    return(summary(res))
    
  } else if(algorithm == "wcc" && type == "a-flexible" && missing(a)){
    
    res <- NA
    
    for (i in 1:rep) {
      
      output_test <- wcc_algorithm_data_2(data=data, a1 = a1, r = r, xvars = xvars) 
      wcc_size <- nrow(output_test$subsample_wcc)
      res[i] <- wcc_size
      
    }
    
    return(summary(res))
    
  } else if(algorithm == "lcc" && missing(a1)  && missing(type) && missing(r)){
    res <- NA
    
    for (i in 1:rep) {
      
      output_test <- lcc_algorithm_V2_data(data=data, a_wcc = a, xvars=xvars) 
      lcc_size <- nrow(output_test$subsample_lcc)
      res[i] <- lcc_size
      
    }
    
    return(summary(res))
    
  }else{
    return("not valid")
  }
  
  
  
}


#Stratified bootstrapp
bootstrap_strat <- function(class_1 = NULL
                            , class_0 = NULL
                            , n_samples = NULL
                            ){
  
  bootstrap_samples <- list()
  
  for (i in 1:n_samples) {
    
    # sampling with replacement (?)
    class_1_bootstr <- class_1[sample(nrow(class_1), replace = TRUE), ]
    class_0_bootstr <- class_0[sample(nrow(class_0), replace = TRUE), ]
    
    ind_sample <- rbind(class_1_bootstr, class_0_bootstr)
    bootstrap_samples[[i]] <- ind_sample
  
    }
  return(bootstrap_samples)
}


# Stratified subsampling 
stratified_subsample <- function(data, y, p0, p1, b) {
  # data = 
  # y 
  # p0 = prop of class 0
  # p1 = prop of class 1
  # b = block size as in as in Politis, Romano, Wolf (1999)
  
  split_data <- split(data, y)
  
  subsample_size <- round(c(p0, p1) * b)
  
  subsample <- do.call(rbind,
                       mapply(function(x, b) x[sample(nrow(x), min(b, nrow(x))), ],
                              split_data, subsample_size, SIMPLIFY = FALSE))
  
  return(subsample)
}


