#' Author: Carolina Alvarez
#' Code with functions used in the simulation study
#' 
#' 
#' Data Generation Process following Fithian and Hastie. Flexible function for creating an imbalanced data set.
#' 
#' @param N: Integer, size of desired population
#' @param r: Imbalanced ratio, as the rate of class 0, P(Y=0|X=x)
#' @param distribution: Distribution of predictors. Options: Gaussian.
#' @param k : Length of predictor vector.
#' @param mean1 : mean of class 1
#' @param mean0: mean of class 0
#' @param sigma1: variance-covariance of predictors of class1
#' @param sigma0: variance-covariance of predictors of class0
#' 
#' @returns df: imbalanced population

gdp.imbalanced <- function(N
                           , r
                           , distribution
                           , k
                           , mean1
                           , mean0
                           , sigma1
                           , sigma0) {
  
  
  n_class1 <- ceiling(N * (1-r))
  n_class0 <- ceiling(N * r)
  
  y0 <- rep(0, n_class0)
  y1 <- rep(1, n_class1)
  
  if (distribution=="gaussian"){
    X_class1 <- matrix(mvrnorm(n_class1, mu = mean1, Sigma = sigma1, empirical = TRUE), ncol = k)
    X_class1 <- cbind(y1, X_class1)
    
    X_class0 <- matrix(mvrnorm(n_class0, mu = mean0, Sigma = sigma0, empirical = TRUE), ncol = k)
    X_class0 <- cbind(y0, X_class0)
    
    df <- as.data.frame(rbind(X_class1, X_class0))
    
    colnames(df) <- c("y", paste0("X", 1:k))
    
    return(df)
    
  }
  else{
    stop("Distribution not allowed.")
  }
  
}


get.true.intercept <- function(r, x0, betas){
  #' The function allows to recover the true intercept for a given value of r 
  #' the complexity of generating the simulated dataset specifically for fixed 
  #' percentages of r requires b0 values to vary for different r
  #' 
  #' r (numeric, percentage): imbalanced ratio, % of '1'
  #' x0 (numeric or vector): fixed value for X_k
  #' betas (numeric or vector): fixed value for beta_k
  
  
  beta_0 <- log(r) - log(1-r) - t(x0)%*%betas
  return(as.numeric(beta_0))
  
}

get.true.intercept(0.02, c(0.5, 0.5, 0.5), c(1, 1, 1))

#' Case-Control subsampling by Fithian and Hastie 
#'
#' Algorithm for subsampling and fitting a logistic regression in the data
#' hyperparameter a(y): the proportion of 1's we want to subsample. The subsample is generated by first generating 
#' ui ~ U(0,1) that is mutually intedepndent from the pilot (?), the data and each other. Then zi ~ 1 if ui<=a(yi)


cc_algorithm <- function(data, a){
  
  k <- length(data) - 1 # we take "y" out
  
  selection_bias <- log(a/(1-a))
  
  prob_function <- function(data, a){
    
    data$a <- ifelse(data$y == 0, 1 - a, a)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, a)
  
  U <- runif(nrow(data), 0, 1)
  tmp01$U <- U
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  #Subsample
  tmp02 <- tmp01[tmp01$Z==1, ] 
  class_distr_subsample <- table(tmp02$y)
  
  xvars <- paste("X", 1:k, sep="")
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = binomial) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  beta0_adjusted <- coef_unadjusted[1] - selection_bias
  
  coef_adjusted <- c(beta0_adjusted, coef_unadjusted[2:(k+1)])
  
  res <- list("subsample_cc" = tmp02
              , "coef_unadjusted" = coef_unadjusted
              , "coef_adjusted" = coef_adjusted
              )
  
  return(res)
  
}

cc_algorithm_Ns <- function(data, a){
  
  k <- length(data) - 1 # we take "y" out
  
  selection_bias <- log(a/(1-a))
  
  xvars <- paste("X", 1:k, sep="")
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= data
                         , family = binomial) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  beta0_adjusted <- coef_unadjusted[1] - selection_bias
  
  coef_adjusted <- c(beta0_adjusted, coef_unadjusted[2:(k+1)])
  
  res <- list("subsample_S" = data
              , "coef_unadjusted" = coef_unadjusted
              , "coef_adjusted" = coef_adjusted
  )
  
  return(res)
  
}


#' Prediction function
#' A flexible function of predicting odds logistic regression framework
#' 
logit_predict <- function(data, names.use, betas){
  # data (dataframe): test set
  # names.use (vector): vector containing strings with names of features (columns of 'data')
  # betas (vector): vector containing beta coefficients, including intercept (IMP for Fithian and Hastie (2014))
  # For cc, betas is the vector of adjusted coef
  
  X <- as.matrix(cbind(rep(1), data[, names.use]))
  colnames(X) <- c("intercept", names.use)
  n <- nrow(X)
  p <- ncol(X)
  p1 <- seq_len(p) 
  beta <- betas 
  predictor_vector <- 1/(1+exp((-1)*drop(X[, p1, drop = FALSE] %*% beta[p1])))
  return(predictor_vector)
  
}




#' Stratified hold-out method / Generation of train and test sets
#' Function assures that class distribution p(x) of 'y' remains

strat_sampling <- function(data, split_criteria){
  #' data (dataframe): data to split into train and test set
  #' split_criteria (numeric): percentage of observations assigned to the train set
  
  train_idx <- c()
  test_idx <- c()
  
  for (i in c('0', '1')) {
    
    idx <- which(data$y == i)
    n_train <- round(length(idx) * split_criteria) 
    
    train_idx_i <- sample(idx, n_train)
    test_idx_i <- setdiff(idx, train_idx_i)
    train_idx <- c(train_idx, train_idx_i)
    test_idx <- c(test_idx, test_idx_i)
  
    }
  
  # Create train and test sets
  df_train <- data[train_idx, ]
  df_test <- data[test_idx, ]
  
  res <- list("df_train" = df_train
              , "df_test" = df_test)
  
  return(res)
  
}

# Proving I get the same prob and ratios as equations for CC

a_bar <- function(a, r){
  a_bar <- a*(1-r) + (1-a) * r
  return(a_bar)
}

prop_Ps <-function(a, r){
  a1 <- a
  a0 <- 1-a
  
  prop_Ps_1 <- (a1 * (1-r))/ (a1*(1-r) + a0*r)
  prop_Ps_0 <- (a0 * r)/ (a1*(1-r) + a0*r)
  
  c <- c(prop_Ps_0, prop_Ps_1)
  
  return(c)
}


# Function for taking N_s


gdp.imbalanced.Ns <- function(a
                              , r
                              , distribution
                              , Ns_size
                              , k
                              , mean1
                              , mean0
                              , sigma1
                              , sigma0){

  prop_Ps <- prop_Ps(a, r)
  
  prop_Ps_1 <- as.numeric(prop_Ps[2])
  prop_Ps_1 <- round(prop_Ps_1, 1)
  
  prop_Ps_0 <- as.numeric(prop_Ps[1])
  prop_Ps_0 <- round(prop_Ps_0, 1)
  
  N <- Ns_size
  
  # now the usual dgp
  n_class1 <- ceiling(N * prop_Ps_1)
  n_class0 <- ceiling(N * prop_Ps_0)
  
  y0 <- rep(0, n_class0)
  y1 <- rep(1, n_class1)
  
  if (distribution=="gaussian"){
    
    X_class1 <- matrix(mvrnorm(n_class1, mu = mean1, Sigma = sigma1, empirical = TRUE), ncol = k)
    X_class1 <- cbind(y1, X_class1)
    
    X_class0 <- matrix(mvrnorm(n_class0, mu = mean0, Sigma = sigma0, empirical = TRUE), ncol = k)
    X_class0 <- cbind(y0, X_class0)
    
    df <- as.data.frame(rbind(X_class1, X_class0))
    
    colnames(df) <- c("y", paste0("X", 1:k))
    
    return(df)
    
  }
  else{
    stop("Distribution not allowed.")
  }
  
  
}


# Function for WCC sampling 
# 1st. Trial: Not adjusting the coefficient of the intercept (it does not seem like they do...)


wcc_algorithm <- function(data, a){
  
  k <- length(data) - 1 # we take "y" out
  
  selection_bias <- log(a/(1-a))
  
  prob_function <- function(data, a){
    
    data$a <- ifelse(data$y == 0, 1 - a, a)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, a)
  
  U <- runif(nrow(data), 0, 1)
  tmp01$U <- U
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  #Subsample
  tmp02 <- tmp01[tmp01$Z==1, ] 
  class_distr_subsample <- table(tmp02$y)
  
  #weights vector
  tmp02$w <- 1/tmp02$a
  
  
  xvars <- paste("X", 1:k, sep="")
  
  # https://github.com/alan-turing-institute/PosteriorBootstrap/issues/16 on why to use quasibinomial instead of binomial
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = quasibinomial()
                         , weights = w) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  res <- list("subsample_wcc" = tmp02
              , "coef_unadjusted" = coef_unadjusted
  )
  
  return(res)
  
}
  


wcc_algorithm_Ns <- function(data, a){
  
  k <- length(data) - 1 # we take "y" out
  
  selection_bias <- log(a/(1-a))
  
  prob_function <- function(data, a){
    
    data$a <- ifelse(data$y == 0, 1 - a, a)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, a)
  
  #weights vector
  tmp01$w <- 1/tmp01$a
  
  
  xvars <- paste("X", 1:k, sep="")
  
  # https://github.com/alan-turing-institute/PosteriorBootstrap/issues/16 on why to use quasibinomial instead of binomial
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp01
                         , family = quasibinomial()
                         , weights = w) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  res <- list("subsample_S" = tmp01
              , "coef_unadjusted" = coef_unadjusted
  )
  
  return(res)
  
}


lcc_algorithm <- function(data_train, a_wcc){
  
  k <- length(data_train) - 1 # we take "y" out
  
  #here, despite using the strat_sampling function that is built to separate 
  #into train and test, we use this split as two training sets for each algorithm.
  # The only thing i want to make sure is that the split is stratified.
  data_split <- strat_sampling(data_train, 0.5)
  data_wcc <- data_split$df_train
  data_lcc <- data_split$df_test
  
  #run the pilot
  wcc_output <- wcc_algorithm(data_wcc, a_wcc)
  subsample_pilot <- wcc_output$subsample_wcc
  coef_unadjusted_wcc <- wcc_output$coef_unadjusted
  
  #predict on LCC data
  y_hat <- logit_predict(data_lcc, c(paste0("X", 1:k)), coef_unadjusted_wcc)
  
  prob_function <- function(data, y_hat){
    
    data$a <- ifelse(data$y == 0, 1 - a, a)
    data$a <- ifelse(data$y == 0, y_hat, 1-y_hat)
    
    return(data)
  }
  
  tmp01 <- prob_function(data_lcc, y_hat)
  
  U <- runif(nrow(tmp01), 0, 1) # TO DO: in CC instead of tmp01 I wrote data... same?
  
  tmp01$U <- U
  
  a_bar_lcc <- tmp01$a
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  #Subsample
  tmp02 <- tmp01[tmp01$Z==1, ] 
  
  xvars <- paste("X", 1:k, sep="")
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = binomial) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted_lcc <- as.vector(model_subsample$coefficients)
  
  # Now, all coeficients get adjusted, not just the intercept
  coef_adjusted_lcc <- coef_unadjusted_lcc + coef_unadjusted_wcc
  
  
  res <- list("subsample_lcc" = tmp02
              , "subsample_pilot" = subsample_pilot
              , "coef_unadjusted" = coef_unadjusted_lcc
              , "coef_adjusted" = coef_adjusted_lcc
              , "a_bar_lcc" = a_bar_lcc
  )

  return(res)

}



### Functions with fixed subsample size

cc_algorithm_fixed <- function(data, a, ns_fixed){
  
  k <- length(data) - 1 # we take "y" out
  
  selection_bias <- log(a/(1-a))
  
  prob_function <- function(data, a){
    
    data$a <- ifelse(data$y == 0, 1 - a, a)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, a)
  
  U <- runif(nrow(data), 0, 1)
  tmp01$U <- U
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  #Subsample
  tmp02 <- tmp01[tmp01$Z==1, ] 
  
  # Ensure fixed subsample has the same proportions of 1 and 0 as tmp02
  n_1 <- sum(tmp02$y == 1)
  n_0 <- sum(tmp02$y == 0)
  prop_1 <- n_1 / (n_1 + n_0)
  
  # Set fixed subsample size to 1000
  ns_fixed_1 <- round(ns_fixed * prop_1)
  ns_fixed_0 <- ns_fixed - ns_fixed_1
  
  # Create fixed subsample with unique data points
  idx_1 <- sample.int(n_1, ns_fixed_1, replace = FALSE)
  idx_0 <- sample.int(n_0, ns_fixed_0, replace = FALSE)
  
  tmp02_fixed <- rbind(tmp02[tmp02$y == 1, ][idx_1, ],
                       tmp02[tmp02$y == 0, ][idx_0, ])
  
  xvars <- paste("X", 1:k, sep="")
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = binomial) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  beta0_adjusted <- coef_unadjusted[1] - selection_bias
  
  coef_adjusted <- c(beta0_adjusted, coef_unadjusted[2:(k+1)])
  
  res <- list("subsample_cc" = tmp02_fixed
              , "coef_unadjusted" = coef_unadjusted
              , "coef_adjusted" = coef_adjusted
  )
  
  return(res)
  
}


cc_algorithm_fixed2 <- function(data, r, a, ns_fixed){
  
  k <- length(data) - 1 # we take "y" out
  
  selection_bias <- log(a/(1-a))
  
  prob_function <- function(data, a){
    
    data$a <- ifelse(data$y == 0, 1 - a, a)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, a)
  
  U <- runif(nrow(data), 0, 1)
  tmp01$U <- U
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  # Directly sample from data to generate tmp02_fixed
  n <- nrow(data)
  a1 <- a
  a0 <- 1-a
  prop_1_fixed <- (a1 * (1-r))/ (a1*(1-r) + a0*r)
  
  n_1 <- round(prop_1_fixed * ns_fixed)
  n_0 <- ns_fixed - n_1
  
  idx_1 <- sample(which(tmp01$y == 1), n_1, replace = FALSE)
  idx_0 <- sample(which(tmp01$y == 0), n_0, replace = FALSE)
  
  tmp02_fixed <- rbind(data[idx_1, ],
                       data[idx_0, ])
  
  xvars <- paste("X", 1:k, sep="")
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = binomial) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  beta0_adjusted <- coef_unadjusted[1] - selection_bias
  
  coef_adjusted <- c(beta0_adjusted, coef_unadjusted[2:(k+1)])
  
  res <- list("subsample_cc" = tmp02_fixed
              , "coef_unadjusted" = coef_unadjusted
              , "coef_adjusted" = coef_adjusted
  )
  
  return(res)
  
}
      