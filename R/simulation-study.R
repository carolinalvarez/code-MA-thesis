#' Author: Carolina Alvarez
#' Date: 16.02.2023
#' 
#' 
#' Data Generation Process following Fithian and Hastie 
#'
#' Generates data set that is flexible in amount of imbalance, size of training set and class overlapping 
#' (?) also distribution from which coufounders are drawn 
#' I think this will make the class overlapping flexible in the mean parameters?
#' parameter c: the proportion of 0's we want in the data


gdp.imbalanced <- function(N
                           , r
                           , distribution
                           , k
                           , mean1
                           , mean0
                           , sd1
                           , sd0) {
  
  n_class1 <- ceiling(N * (1-r))
  n_class0 <- ceiling(N * r)
  
  y0 <- rep(0, n_class0)
  y1 <- rep(1, n_class1)
  
  if (distribution=="gaussian"){
    X_class1 <- replicate(k, rnorm(n_class1, mean = mean1, sd=sd1))
    X_class1 <- cbind(y1, X_class1)
    
    X_class0 <- replicate(k, rnorm(n_class0, mean = mean0, sd = sd0))
    X_class0 <- cbind(y0, X_class0)
    
    df <- as.data.frame(rbind(X_class1, X_class0))
    
    colnames(df) <- c("y", paste0("X", 1:k))
    
    return(df)
    
  }
  else{
    stop("Distribution not allowed.")
  }
  
}


get.true.intercept <- function(r, x0, betas){
  #' The function allows to recover the true intercept for a given value of r 
  #' the complexity of generating the simulated dataset specifically for fixed 
  #' percentages of r requires b0 values to vary for different r
  #' 
  #' r (numeric, percentage): imbalanced ratio, % of '1'
  #' x0 (numeric or vector): fixed value for X_k
  #' betas (numeric or vector): fixed value for beta_k
  
  
  beta_0 <- log(r) - log(1-r) - t(x0)%*%betas
  return(beta_0)
  
}

get.true.intercept(0.02, c(0.5, 0.5, 0.5), c(1, 1, 1))

#' Case-Control subsampling by Fithian and Hastie 
#'
#' Algorithm for subsampling and fitting a logistic regression in the data
#' hyperparameter a(y): the proportion of 1's we want to subsample. The subsample is generated by first generating 
#' ui ~ U(0,1) that is mutually intedepndent from the pilot (?), the data and each other. Then zi ~ 1 if ui<=a(yi)


cc_algorithm <- function(data, c){
  k <- length(data) - 1 #we take "y" out
  
  selection_bias <- log(c/(1-c))
  
  prob_function <- function(data, c){
    
    data$a <- ifelse(data$y == 0, 1 - c, c)
    
    return(data)
  }
  
  tmp01 <- prob_function(data, c)
  
  U <- runif(nrow(data), 0, 1)
  tmp01$U <- U
  
  tmp01$Z <- NA
  tmp01$Z <- ifelse(tmp01$U <= tmp01$a, 1, 0)
  
  tmp02 <- tmp01[tmp01$Z==1, ] #subsample
  
  xvars <- paste("X", 1:k, sep="")
  
  model_subsample <- glm(as.formula(paste("y ~ ", paste(xvars, collapse= "+")))
                         , data= tmp02
                         , family = binomial) #imp: remove a to avoid perfect separation and convergence issues
  
  coef_unadjusted <- as.vector(model_subsample$coefficients)
  
  beta0_adjusted <- coef_unadjusted[1] - selection_bias
  
  coef_adjusted <- c(beta0_adjusted, coef_unadjusted[2:(k+1)])
  
  res <- list("subsample_S" = tmp02
              , "coef_unadjusted" = coef_unadjusted
              , "coef_adjusted" = coef_adjusted)
  
  return(res)
  
}

logit_predict <- function(data, names.use, betas){
  # data (dataframe): test set
  # names.use (vector): vector containing strings with names of features (columns of 'data')
  # betas (vector): vector containing beta estimators (IMP for Fithian and Hastie (2014))
  
  X <- as.matrix(cbind(rep(1), data[, names.use]))
  colnames(X) <- c("intercept", names.use)
  n <- nrow(X)
  p <- ncol(X)
  p1 <- seq_len(p) 
  beta <- betas 
  predictor_vector <- 1/(1+exp((-1)*drop(X[, p1, drop = FALSE] %*% beta[p1])))
  return(predictor_vector)
  
}


###############################################################################


set.seed(123)

# Train sets and model training
df <- gdp.imbalanced(N = 10000, r = 0.99, distribution= "gaussian", k=5, mean1=0.4, mean0=0, sd1=1, sd0=1)
table(df$y)

cc_output <- cc_algorithm(df, 0.8)
tmp02 <- cc_output$subsample_S
coef_unadjusted <- cc_output$coef_unadjusted
coef_adjusted <- cc_output$coef_adjusted

model_full <- glm("y~.", data = df, family = binomial)
coef_full <- model_full$coefficients



# Test sets and prediction
set.seed(113)
df_test <- gdp.imbalanced(N = 10000, r = 0.99, distribution= "gaussian", k=5, mean1=0.4, mean0=0, sd1=1, sd0=1)
table(df_test$y)

cc_output_test <- cc_algorithm(df_test, 0.8)
tmp02_test <- cc_output_test$subsample_S


#y_hat_df <- predict(model_full, newdata = df_test, type = 'response')
y_hat_df <- logit_predict(df_test, c("X1", "X2", "X3", "X4", "X5"), coef_full)
y_hat_subsample <- logit_predict(tmp02_test, c("X1", "X2", "X3", "X4", "X5"), coef_adjusted)


library(pROC)

test_roc_df = roc(df_test$y ~ y_hat_df, plot = TRUE, print.auc = TRUE)
test_roc_df$auc
test_roc_s = roc(tmp02_test$y ~ y_hat_subsample, plot = TRUE, print.auc = TRUE)
test_roc_s$auc





